def getTransId(ts:String, d_store_id:Int, d_lane:Int, d_trans_seq:Int): Long = {
   val daydate = ts.replaceAll("-","").split(" ")(0)
   val store_id = "%05d".format(d_store_id)
   val lane = "%02d".format(d_lane)
   val trans_seq = "%04d".format(d_trans_seq)

   (daydate + store_id.toString + lane.toString + trans_seq.toString).toLong
}

val transId = getTransId(_,_,_,_)

sqlContext.udf.register("getTransId", transId)

sqlContext.sql("SELECT getTransId(1,23,78,998)").show()


*****************************************************************

val df = Seq(
  (0, "hello"),
  (1, "world")).toDF("id", "text")

// Define a "regular" Scala function
// It's a clone of upper UDF
val toUpper: String => String = _.toUpperCase

import org.apache.spark.sql.functions.udf
val upper = udf(toUpper)

scala> df.withColumn("upper", upper('text)).show
+---+-----+-----+
| id| text|upper|
+---+-----+-----+
|  0|hello|HELLO|
|  1|world|WORLD|
+---+-----+-----+

// You could have also defined the UDF this way
val upperUDF = udf { s: String => s.toUpperCase }

// or even this way
val upperUDF = udf[String, String](_.toUpperCase)

scala> df.withColumn("upper", upperUDF('text)).show
+---+-----+-----+
| id| text|upper|
+---+-----+-----+
|  0|hello|HELLO|
|  1|world|WORLD|
+---+-----+-----+