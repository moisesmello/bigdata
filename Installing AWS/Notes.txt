Machine Details
***************************************
Namenode
Publ. 18.222.127.208
Priv. 172.31.24.122
ec2-18-222-127-208.us-east-2.compute.amazonaws.com
White


datanode 1
Publ. 18.224.172.141
Priv. 172.31.20.187
ec2-18-224-172-141.us-east-2.compute.amazonaws.com
Green


datanode 2
Publ. 18.221.7.149
Priv. 172.31.29.141
ec2-18-221-7-149.us-east-2.compute.amazonaws.com


datanode 3
Publ. 18.222.5.94
Priv. 172.31.17.83
ec2-18-222-5-94.us-east-2.compute.amazonaws.com



***************************************
# Edit the bashrc fil
vi ~/.bash

# Go to the last line
:$ 

***************************************
-- Installation sequence

sudo apt-get updated

# check available versions of Java
java

# Install java (headless = core)
sudo apt-get -y install openjdk-8-jre-headless

# check the installation
java -version

# Download Hadoop
sudo wget http://archive.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz

# Extract file
tar xvzf hadoop-2.7.3.tar.gz

# Rename folder
mv hadoop-2.7.3 hadoop

# Locate bash file
vi ~/.bashrc

# Paste config

#====== HADOOP VAR =======#
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_INSTALL=/home/ubuntu/hadoop
export PATH=$PATH:$HADOOP_INSTALL/bin
export PATH=$PATH:$HADOOP_INSTALL/sbin
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL
export YARN_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib/native"

# Apply config
source ~/.bashrc

# Test Hadoop
hadoop

# Go to the config dirs
cd hadoop/etc/hadoop

# Modify the hadoop-env.sh file to set the Java path
sudo vi hadoop-env.sh
	JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

# Modify the site-core.xml file to set the namenode address
vi core-site.xml

<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value><DNS of namenode>:9000</value>
  </property>
</configuration>

# Modify hdfs-site.xml NAMENODE
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///usr/local/hadoop/hdfs/data</value>
   </property>
</configuration>

# Modify hdfs-site.xml DATANODE
<configuration>
<property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///usr/local/hadoop/hdfs/data</value>
  </property>
</configuration>

# Prepare mapred-site.xml NAMENODE
cp mapred-site.xml.template mapred-site.xml

# Modify mapred-site.xml NAMENODE
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>

# Yarn-site.xml NAMENODE
<configuration>
  <!-- Site specific YARN configuration properties -->
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value><DNS of namenode></value>
  </property>
</configuration>

# Create multiple instances of the datanode

# Configuration of the SSH

# Generate Key on the NAMENODE (don't enter a name, go with the default options)
# The key will be generated in /home/ubuntu/.ssh/
ssh-keygen

# Copy the content of file /home/ubuntu/.ssh/id_rsa.pub to clipboard

# Paste that contents in the file  ~/.ssh/authorized_keys in each DATANODE

# Create a new file in /home/ubuntu/.ssh/ called config (~/.ssh/config) and add this content:
# This will enable accessing those servers by their names (SSH)

Host namenode
  HostName <DNS of namenode>
  User ubuntu
  IdentityFile ~/.ssh/id_rsa

Host datanode1
  HostName <DNS of datanode1>
  User ubuntu
  IdentityFile ~/.ssh/id_rsa

Host datanode2
  HostName <DNS of datanode2>
  User ubuntu
  IdentityFile ~/.ssh/id_rsa

Host datanode3
  HostName <DNS of datanode3>
  User ubuntu
  IdentityFile ~/.ssh/id_rsa

# Check connectivity
ssh namenode
ssh datanode1
ssh datanode2
ssh datanode3

# Go to Hadoop config files
cd /home/ubuntu/hadoop/etc/hadoop

# Modify slaves file and add the contents: NAMENODE
vi slaves
	<DNS of datanode1>
	<DNS of datanode2>
	<DNS of datanode3>

# Modify masters file and add the contents: NAMENODE
vi masters
	<DNS of namenode>

# Format NAMENODE
hdfs namenode -format

# Start service (HDFS)
start-dfs.sh

# Start Yarn
start-yarn.sh

# Verify installation, if prompt to install Java JDK, go ahead and install it
jps
